"""
Servicio de Generaci√≥n para el Sistema RAG
==========================================

Este m√≥dulo implementa la funcionalidad de generar respuestas usando LLMs
bas√°ndose en el contexto recuperado del sistema RAG.

TAREAS SEMANA 2:
- Inicializar LLM (ChatGoogleGenerativeAI)
- Crear prompt template para RAG
- Implementar generaci√≥n de respuestas con contexto recuperado
- Implementar evaluaci√≥n con RAGAS

TAREAS SEMANA 3:
- Implementar query rewriting (reescritura de consultas)

TUTORIALES:
- RAG paso a paso, parte 3: recuperaci√≥n y generaci√≥n de respuestas
- Reescritura de consultas
"""

import time
import re
from typing import Dict, Any, List
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.schema import Document
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
from datasets import Dataset
from app.services.generation_service_local import LocalGenerationService


class GenerationService:
    """
    Servicio para generar respuestas usando Gemini con contexto RAG.
    
    IMPLEMENTACI√ìN REQUERIDA (SEMANA 2):
    - __init__: Inicializar LLM y prompt template
    - generate_response: Generar respuesta con contexto
    - evaluate_with_ragas: Evaluar sistema con m√©tricas RAGAS
    
    IMPLEMENTACI√ìN REQUERIDA (SEMANA 3):
    - rewrite_query: Reescribir consultas para mejorar recuperaci√≥n
    """
    
    def __init__(
        self, 
        model: str = "gemini-2.5-flash",
        temperature: float = 0.7,
        max_tokens: int = 1024,
        use_local: bool = False
    ):
        """
        Inicializa el servicio de generaci√≥n con Gemini o modelo local.
        
        TODO SEMANA 2:
        - Inicializar self.llm con ChatGoogleGenerativeAI
        - Crear self.prompt con ChatPromptTemplate.from_template()
        - El prompt debe tener placeholders: {question} y {context}
        
        RECOMENDACIONES PARA EL PROMPT:
        - Instruir al modelo a responder solo con el contexto proporcionado
        - Indicar qu√© hacer si no tiene informaci√≥n suficiente
        - Mantener respuestas concisas
        
        Args:
            model: Nombre del modelo de Google AI
            temperature: Temperatura para generaci√≥n (0-1)
            max_tokens: N√∫mero m√°ximo de tokens en respuesta
            use_local: Si True, usa modelo local en lugar de Google AI
        """
        self.model_name = model
        self.use_local = use_local
        
        if use_local:
            # Usar modelo local directamente
            print("ü§ñ Usando modelo local para generaci√≥n")
            self.local_generator = LocalGenerationService()
            self.llm = None
            self.prompt = None
        else:
            try:
                # Intentar usar Google AI primero
                print(f"ü§ñ Intentando usar Google AI: {model}")
                self.llm = ChatGoogleGenerativeAI(
                    model=model,
                    temperature=temperature,
                    max_output_tokens=max_tokens
                )
                
                print("‚úÖ Google AI inicializado correctamente")
                
                # Crear prompt template para RAG - texto plano sin saltos de l√≠nea
                self.prompt = ChatPromptTemplate.from_template(
                    "Eres un asistente especializado en responder preguntas bas√°ndote √∫nicamente en el contexto proporcionado. "
                    "CONTEXTO: {context} "
                    "PREGUNTA: {question} "
                    "INSTRUCCIONES: "
                    "1. Responde la pregunta usando SOLO la informaci√≥n del contexto proporcionado. "
                    "2. Si el contexto no contiene informaci√≥n suficiente para responder, di claramente: No tengo informaci√≥n suficiente en los documentos para responder esta pregunta. "
                    "3. Mant√©n tu respuesta concisa y directa. "
                    "4. Si mencionas informaci√≥n espec√≠fica, indica de qu√© documento proviene. "
                    "5. No inventes informaci√≥n que no est√© en el contexto. "
                    "RESPUESTA:"
                )
                
            except Exception as e:
                error_msg = str(e)
                if "quota" in error_msg.lower() or "429" in error_msg:
                    print(f"‚ö†Ô∏è Cuota de Google AI excedida: {error_msg}")
                    print("üîÑ Cambiando autom√°ticamente a modelo local...")
                else:
                    print(f"‚ö†Ô∏è Error con Google AI: {error_msg}")
                    print("üîÑ Cambiando a modelo local...")
                
                self.local_generator = LocalGenerationService()
                self.llm = None
                self.prompt = None
                self.use_local = True
        
        print(f"ü§ñ GenerationService inicializado con modelo: {model}")
        print(f"üå°Ô∏è Temperatura: {temperature}, Max tokens: {max_tokens}")
    
    def generate_response(
        self, 
        question: str, 
        retrieved_docs: List[Document]
    ) -> Dict[str, Any]:
        """
        Genera una respuesta usando el contexto recuperado.
        
        TODO SEMANA 2:
        - Preparar contexto: concatenar retrieved_docs.page_content
        - Construir mensaje con self.prompt.invoke({question, context})
        - Generar respuesta con self.llm.invoke(messages)
        - Extraer archivos consultados de los metadatos
        - Retornar diccionario con:
          * "answer": texto de la respuesta
          * "sources": lista de archivos consultados
          * "context": documentos recuperados
        
        NOTA: Este m√©todo es llamado por ask.py despu√©s de recuperar documentos
   
        
        Args:
            question: Pregunta del usuario
            retrieved_docs: Documentos recuperados del vector store
            
        Returns:
            Dict con answer, sources y context
        """
        try:
            print(f"ü§ñ Generando respuesta para: '{question[:50]}...'")
            print(f"üìö Contexto disponible: {len(retrieved_docs)} documentos")
            
            # Preparar contexto concatenando documentos como texto plano sin saltos de l√≠nea
            context_parts = []
            sources = set()
            
            for i, doc in enumerate(retrieved_docs):
                source_file = doc.metadata.get('source_file', f'documento_{i+1}')
                sources.add(source_file)
                
                # Limpiar el contenido del documento: texto plano sin caracteres especiales problem√°ticos
                doc_content = doc.page_content
                
                # Reemplazar comillas dobles y simples que podr√≠an interferir con el prompt
                doc_content = doc_content.replace('"', "'").replace('"""', "'''")
                
                # Normalizar saltos de l√≠nea primero
                doc_content = doc_content.replace('\r\n', ' ').replace('\r', ' ').replace('\n', ' ')
                
                # Reemplazar m√∫ltiples espacios en blanco por un solo espacio
                doc_content = re.sub(r'\s+', ' ', doc_content)
                
                # Eliminar caracteres de control
                doc_content = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', doc_content)
                
                # Formatear cada documento con su fuente (texto plano sin saltos de l√≠nea)
                context_part = f"Documento {i+1} - {source_file}: {doc_content}"
                context_parts.append(context_part)
            
            # Unir contexto como texto plano simple sin saltos de l√≠nea
            context = ". ".join(context_parts)
            
            # Limpiar el contexto final de caracteres problem√°ticos adicionales
            context = context.strip()
            
            # Reemplazar cualquier salto de l√≠nea restante por espacios
            context = context.replace('\n', ' ').replace('\r', ' ')
            context = re.sub(r'\s+', ' ', context)
            
            print(f"üìÑ Archivos consultados: {list(sources)}")
            print(f"üìù Tama√±o del contexto: {len(context)} caracteres")
            
            if self.use_local or self.llm is None:
                # Usar modelo local
                print("ü§ñ Usando modelo local para generaci√≥n...")
                return self.local_generator.generate_response(question, retrieved_docs)
            else:
                # Usar Google AI
                try:
                    # Construir el mensaje completo como texto plano (igual que funciona en Postman)
                    full_message = (
                        "Eres un asistente especializado en responder preguntas bas√°ndote √∫nicamente en el contexto proporcionado. "
                        f"CONTEXTO: {context} "
                        f"PREGUNTA: {question} "
                        "INSTRUCCIONES: "
                        "1. Responde la pregunta usando SOLO la informaci√≥n del contexto proporcionado. "
                        "2. Si el contexto no contiene informaci√≥n suficiente para responder, di claramente: No tengo informaci√≥n suficiente en los documentos para responder esta pregunta. "
                        "3. Mant√©n tu respuesta concisa y directa. "
                        "4. Si mencionas informaci√≥n espec√≠fica, indica de qu√© documento proviene. "
                        "5. No inventes informaci√≥n que no est√© en el contexto. "
                        "RESPUESTA:"
                    )
                    
                    # Asegurar que no haya saltos de l√≠nea
                    full_message = full_message.replace('\r\n', ' ').replace('\r', ' ').replace('\n', ' ')
                    full_message = re.sub(r'\s+', ' ', full_message).strip()
                    
                    # print(f"CONTENIDO ENVIADO A LA IA (texto plano sin saltos de l√≠nea): {full_message}")
                    
                    # Usar LangChain para enviar el mensaje
                    from langchain_core.messages import HumanMessage
                    message = HumanMessage(content=full_message)
                    
                    print(f"üîß Configuraci√≥n LLM: temperatura={self.llm.temperature}, max_output_tokens={self.llm.max_output_tokens}")
                    print(f"üì® Enviando mensaje con {len(full_message)} caracteres...")
                    
                    try:
                        response = self.llm.invoke([message])
                        
                        # Extraer texto de la respuesta
                        answer = response.content if hasattr(response, 'content') else str(response)
                        
                        print(f"üì• Respuesta recibida: tipo={type(response)}, tiene content={hasattr(response, 'content')}")
                        if hasattr(response, 'content'):
                            print(f"üìÑ Contenido respuesta: '{answer[:100] if answer else 'VAC√çO'}...'")
                        
                        # Verificar si la respuesta est√° vac√≠a (cuota excedida)
                        if not answer or len(answer.strip()) == 0:
                            print("‚ö†Ô∏è Google AI devuelve respuesta vac√≠a")
                            print(f"üîç Debug - respuesta completa: {response}")
                            print(f"üîç Debug - respuesta type: {type(response)}")
                            print(f"üîç Debug - respuesta attrs: {dir(response)}")
                            # No activar fallback autom√°tico, solo mostrar el error
                            raise ValueError("Google AI devolvi√≥ respuesta vac√≠a")
                        
                        print(f"‚úÖ Respuesta generada: {len(answer)} caracteres")
                        
                        return {
                            "answer": answer,
                            "sources": list(sources),
                            "context": retrieved_docs,
                            "context_length": len(context),
                            "answer_length": len(answer)
                        }
                        
                    except Exception as invoke_error:
                        print(f"‚ùå Error durante invoke: {str(invoke_error)}")
                        print(f"üîç Tipo de error: {type(invoke_error)}")
                        import traceback
                        traceback.print_exc()
                        raise
                    
                except Exception as e:
                    error_msg = str(e)
                    if "quota" in error_msg.lower() or "429" in error_msg:
                        print(f"‚ö†Ô∏è Cuota de Google AI excedida durante generaci√≥n: {error_msg}")
                        print("üîÑ Cambiando autom√°ticamente a modelo local...")
                        
                        # Cambiar a modelo local y usar fallback
                        self.local_generator = LocalGenerationService()
                        self.use_local = True
                        return self.local_generator.generate_response(question, retrieved_docs)
                    else:
                        raise e
            
        except Exception as e:
            print(f"‚ùå Error generando respuesta: {str(e)}")
            return {
                "answer": f"Error generando respuesta: {str(e)}",
                "sources": [],
                "context": retrieved_docs,
                "context_length": 0,
                "answer_length": 0
            }
    
    def evaluate_with_ragas(
        self, 
        dataset: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Eval√∫a el sistema RAG usando m√©tricas RAGAS.
        
        TODO SEMANA 2:
        - Configurar LLM y embeddings para evaluaci√≥n
        - Definir m√©tricas: faithfulness, answer_relevancy, context_precision, context_recall
        - Convertir dataset a formato RAGAS (Dataset.from_dict)
        - Ejecutar evaluate() con dataset y m√©tricas
        - Retornar resultados
        
        DATASET FORMAT:
        [
          {
            "question": "...",
            "answer": "...",
            "contexts": ["...", "..."],
            "ground_truth": "..."
          },
          ...
        ]
   
        
        Args:
            dataset: Lista de ejemplos con question, answer, contexts, ground_truth
            
        Returns:
            Resultados de la evaluaci√≥n RAGAS
        """
        try:
            print(f"üìä Iniciando evaluaci√≥n RAGAS con {len(dataset)} ejemplos")
            
            # Configurar LLM y embeddings para evaluaci√≥n
            llm = ChatGoogleGenerativeAI(
                model="gemini-2.5-flash-lite",  # Modelo ligero para evaluaci√≥n
                temperature=0
            )
            
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/embedding-002"
            )
            
            # Definir m√©tricas RAGAS
            metrics = [
                faithfulness,      # Fidelidad: ¬øla respuesta est√° fundamentada en el contexto?
                answer_relevancy,  # Relevancia: ¬øla respuesta es relevante a la pregunta?
                context_precision, # Precisi√≥n: ¬ølos contextos recuperados son relevantes?
                context_recall     # Recuerdo: ¬øse recuperaron todos los contextos necesarios?
            ]
            
            # Convertir dataset a formato RAGAS
            ragas_dataset = Dataset.from_dict({
                "question": [item["question"] for item in dataset],
                "answer": [item["answer"] for item in dataset],
                "contexts": [item["contexts"] for item in dataset],
                "ground_truth": [item["ground_truth"] for item in dataset]
            })
            
            print("üîÑ Ejecutando evaluaci√≥n RAGAS...")
            
            # Ejecutar evaluaci√≥n
            results = evaluate(
                dataset=ragas_dataset,
                metrics=metrics,
                llm=llm,
                embeddings=embeddings
            )
            
            # Procesar resultados
            evaluation_results = {
                "faithfulness": results["faithfulness"],
                "answer_relevancy": results["answer_relevancy"],
                "context_precision": results["context_precision"],
                "context_recall": results["context_recall"],
                "total_samples": len(dataset),
                "evaluation_timestamp": time.time()
            }
            
            print("‚úÖ Evaluaci√≥n RAGAS completada")
            print(f"üìä Resultados:")
            for metric, score in evaluation_results.items():
                if metric not in ["total_samples", "evaluation_timestamp"]:
                    print(f"   ‚Ä¢ {metric}: {score}")
            
            return evaluation_results
            
        except Exception as e:
            print(f"‚ùå Error en evaluaci√≥n RAGAS: {str(e)}")
            return {
                "error": str(e),
                "faithfulness": 0.0,
                "answer_relevancy": 0.0,
                "context_precision": 0.0,
                "context_recall": 0.0
            }
    
    def rewrite_query(self, question: str) -> str:
        """
        Reescribe la consulta del usuario para mejorar la recuperaci√≥n de documentos.
        
        TODO SEMANA 3:
        - Seleccionar una estrategia de query rewriting (ver tutorial)
        - Crear un prompt de sistema apropiado para la estrategia elegida
        - Usar el LLM para generar la consulta mejorada
        - Retornar consulta reescrita
        - Si falla, retornar question original (fallback)
        
        ESTRATEGIAS SUGERIDAS (escoger una o m√°s):
        - Query Expansion: Expandir la consulta con t√©rminos relacionados
        - Query Decomposition: Dividir consultas complejas en subconsultas
        - Query Refinement: Reformular para mayor precisi√≥n
        - Multi-Query: Generar m√∫ltiples variaciones de la consulta
        
        NOTA: Este m√©todo es llamado por ask.py antes de recuperar documentos
        cuando el par√°metro use_query_rewriting=True
        
        Args:
            question: Consulta original del usuario
            
        Returns:
            str: Consulta reescrita y mejorada
        """
        try:
            print(f"üîÑ Reescribiendo consulta: '{question[:50]}...'")
            
            # Crear prompt para query rewriting
            rewrite_prompt = ChatPromptTemplate.from_template("""
Eres un experto en recuperaci√≥n de informaci√≥n. Tu tarea es reescribir la consulta del usuario para mejorar la b√∫squeda de documentos relevantes.

CONSULTA ORIGINAL: {question}

INSTRUCCIONES:
1. Analiza la consulta original y identifica los conceptos clave
2. Expande la consulta con t√©rminos relacionados y sin√≥nimos
3. Reformula la consulta para ser m√°s espec√≠fica y precisa
4. Mant√©n el significado original pero mejora la claridad
5. Si la consulta es muy general, hazla m√°s espec√≠fica
6. Si la consulta es muy espec√≠fica, considera variaciones

ESTRATEGIAS A APLICAR:
- Query Expansion: A√±adir t√©rminos relacionados
- Query Refinement: Mejorar la precisi√≥n
- Sin√≥nimos y variaciones de t√©rminos clave
- Contexto adicional si es necesario

CONSULTA REESCRITA:
""")
            
            # FORZAR USO DEL MODELO LOCAL PARA QUERY REWRITING
            # Esto evita problemas con cuota de Google AI
            print("ü§ñ Usando modelo local para query rewriting...")
            return self._rewrite_query_local(question)
            
        except Exception as e:
            print(f"‚ùå Error en query rewriting: {str(e)}")
            return question
    
    def _rewrite_query_local(self, question: str) -> str:
        """
        Implementaci√≥n b√°sica de query rewriting para modelo local.
        
        Args:
            question: Consulta original
            
        Returns:
            str: Consulta reescrita
        """
        print(f"üîÑ Iniciando query rewriting local para: '{question[:50]}...'")
        
        # Estrategia 1: Reformulaci√≥n directa para consultas comunes
        question_lower = question.lower()
        
        # Patrones espec√≠ficos de reescritura
        if "informaci√≥n importante" in question_lower and "documentos" in question_lower:
            # Patr√≥n: "¬øQu√© informaci√≥n importante contienen estos documentos?"
            rewritten = question.replace("informaci√≥n importante", "contenido relevante y datos significativos")
            rewritten = rewritten.replace("estos documentos", "los documentos disponibles")
            print(f"‚úÖ Consulta reformulada (patr√≥n 1): '{rewritten[:50]}...'")
            return rewritten
        
        if "informaci√≥n" in question_lower and "contienen" in question_lower:
            # Patr√≥n: "¬øQu√© informaci√≥n contienen...?"
            rewritten = question.replace("informaci√≥n", "datos relevantes y contenido")
            rewritten = rewritten.replace("contienen", "incluyen y presentan")
            print(f"‚úÖ Consulta reformulada (patr√≥n 2): '{rewritten[:50]}...'")
            return rewritten
        
        # Estrategia 2: Expansi√≥n con t√©rminos relacionados
        expansion_terms = {
            "informaci√≥n": ["datos", "contenido", "detalles", "especificaciones"],
            "documentos": ["archivos", "textos", "materiales", "recursos"],
            "importante": ["relevante", "significativo", "clave", "esencial"],
            "contienen": ["incluyen", "presentan", "muestran", "describen"]
        }
        
        expanded_terms = []
        for key, synonyms in expansion_terms.items():
            if key in question_lower:
                expanded_terms.extend(synonyms[:2])  # Tomar solo 2 sin√≥nimos
        
        if expanded_terms:
            # Crear consulta expandida
            unique_terms = list(set(expanded_terms))
            expanded_query = f"{question} {' '.join(unique_terms[:2])}"
            print(f"‚úÖ Consulta expandida: '{expanded_query[:50]}...'")
            return expanded_query
        
        # Estrategia 3: Reformulaci√≥n general
        if "¬øQu√©" in question and "?" in question:
            # Reformular preguntas que empiecen con "¬øQu√©"
            rewritten = question.replace("¬øQu√©", "¬øCu√°les son los")
            rewritten = rewritten.replace("?", " y qu√© detalles espec√≠ficos incluyen?")
            print(f"‚úÖ Consulta reformulada (patr√≥n 3): '{rewritten[:50]}...'")
            return rewritten
        
        # Estrategia 4: Agregar contexto espec√≠fico
        context_additions = ["contenido espec√≠fico", "datos relevantes"]
        context_query = f"{question} {' '.join(context_additions)}"
        print(f"‚úÖ Consulta con contexto: '{context_query[:50]}...'")
        return context_query