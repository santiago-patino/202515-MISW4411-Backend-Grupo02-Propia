#!/usr/bin/env python3
"""
Script para ejecutar evaluaci√≥n RAGAS de la Semana 3
====================================================

Este script ejecuta las evaluaciones RAGAS para comparar las mejoras
implementadas en la Semana 3 del proyecto RAG.
"""

import sys
import os
import json
import time
from pathlib import Path

# Agregar el directorio ra√≠z al path
sys.path.append(str(Path(__file__).parent.parent.parent))

from app.services.generation_service import GenerationService
from app.services.retrieval_service import RetrievalService
from app.services.embedding_service import EmbeddingService
from app.services.chunking_service import ChunkingService, ChunkingStrategy

def create_test_dataset():
    """Crear dataset de prueba para evaluaci√≥n RAGAS"""
    
    # Preguntas que S√ç tienen informaci√≥n en los documentos
    questions_with_info = [
        "¬øCu√°les son los requisitos para participar en las convocatorias?",
        "¬øQu√© documentos necesito para postularme?",
        "¬øCu√°l es el monto m√°ximo de financiaci√≥n?",
        "¬øCu√°les son las fechas l√≠mite de presentaci√≥n?",
        "¬øQu√© tipos de proyectos se financian?"
    ]
    
    # Preguntas que NO tienen informaci√≥n en los documentos
    questions_without_info = [
        "¬øCu√°l es el salario de los investigadores?",
        "¬øC√≥mo se calcula el impuesto sobre la renta?",
        "¬øQu√© deportes se practican en la universidad?",
        "¬øCu√°l es el clima en Bogot√°?",
        "¬øQu√© restaurantes hay cerca de la universidad?"
    ]
    
    # Respuestas esperadas (ground truth)
    ground_truths = [
        "Los requisitos incluyen ser investigador colombiano, tener t√≠tulo de doctorado, y presentar propuesta de investigaci√≥n.",
        "Se requieren documentos como CV, propuesta de investigaci√≥n, y certificados acad√©micos.",
        "El monto m√°ximo var√≠a seg√∫n el tipo de convocatoria, pero puede llegar hasta $500 millones de pesos.",
        "Las fechas l√≠mite se especifican en cada convocatoria, generalmente son trimestrales.",
        "Se financian proyectos de investigaci√≥n en ciencias b√°sicas, aplicadas, y desarrollo tecnol√≥gico."
    ]
    
    # Contextos relevantes (para m√©tricas de contexto)
    contexts = [
        ["Los investigadores deben ser colombianos", "Se requiere t√≠tulo de doctorado", "La propuesta debe ser original"],
        ["CV actualizado", "Propuesta de investigaci√≥n", "Certificados acad√©micos"],
        ["Monto m√°ximo $500 millones", "Financiaci√≥n por 24 meses", "Recursos para investigaci√≥n"],
        ["Fechas l√≠mite trimestrales", "Presentaci√≥n en l√≠nea", "Evaluaci√≥n por pares"],
        ["Ciencias b√°sicas", "Ciencias aplicadas", "Desarrollo tecnol√≥gico"]
    ]
    
    return {
        "questions": questions_with_info + questions_without_info,
        "ground_truths": ground_truths + [""] * len(questions_without_info),
        "contexts": contexts + [[]] * len(questions_without_info),
        "answers": [""] * (len(questions_with_info) + len(questions_without_info))
    }

def evaluate_configuration(config_name, use_preprocessing=False, use_reranking=False, use_query_rewriting=False):
    """Evaluar una configuraci√≥n espec√≠fica"""
    
    print(f"\nüîç Evaluando configuraci√≥n: {config_name}")
    print(f"   Preprocesamiento: {use_preprocessing}")
    print(f"   Reranking: {use_reranking}")
    print(f"   Query Rewriting: {use_query_rewriting}")
    
    try:
        # Inicializar servicios
        embedding_service = EmbeddingService()
        retrieval_service = RetrievalService()
        generation_service = GenerationService()
        
        # Crear dataset de prueba
        dataset = create_test_dataset()
        
        # Generar respuestas para cada pregunta
        answers = []
        contexts = []
        
        for i, question in enumerate(dataset["questions"]):
            print(f"   üìù Procesando pregunta {i+1}/{len(dataset['questions'])}: {question[:50]}...")
            
            try:
                # Generar respuesta usando el endpoint
                import requests
                response = requests.post(
                    "http://localhost:8000/api/v1/ask",
                    json={
                        "question": question,
                        "top_k": 5,
                        "collection": "semana3_test_collection_final",
                        "use_reranking": use_reranking,
                        "use_query_rewriting": use_query_rewriting
                    },
                    timeout=30
                )
                
                if response.status_code == 200:
                    data = response.json()
                    answers.append(data.get("answer", ""))
                    contexts.append([doc.get("page_content", "") for doc in data.get("context_docs", [])])
                else:
                    print(f"   ‚ùå Error en pregunta {i+1}: {response.status_code}")
                    answers.append("")
                    contexts.append([])
                    
            except Exception as e:
                print(f"   ‚ùå Error procesando pregunta {i+1}: {str(e)}")
                answers.append("")
                contexts.append([])
        
        # Actualizar dataset con respuestas generadas
        dataset["answers"] = answers
        dataset["contexts"] = contexts
        
        # Calcular m√©tricas RAGAS b√°sicas
        metrics = calculate_basic_metrics(dataset)
        
        print(f"   ‚úÖ Evaluaci√≥n completada para {config_name}")
        return metrics
        
    except Exception as e:
        print(f"   ‚ùå Error en evaluaci√≥n {config_name}: {str(e)}")
        return None

def calculate_basic_metrics(dataset):
    """Calcular m√©tricas b√°sicas sin RAGAS completo"""
    
    # M√©tricas simplificadas basadas en an√°lisis de respuestas
    total_questions = len(dataset["questions"])
    valid_answers = sum(1 for answer in dataset["answers"] if answer and len(answer.strip()) > 10)
    
    # Faithfulness: Respuestas que no contienen informaci√≥n inventada
    faithful_answers = 0
    for i, answer in enumerate(dataset["answers"]):
        if answer and len(answer.strip()) > 10:
            # Verificar si la respuesta es relevante y no inventada
            if any(keyword in answer.lower() for keyword in ["convocatoria", "investigaci√≥n", "financiaci√≥n", "requisitos"]):
                faithful_answers += 1
    
    # Context Precision: Respuestas con contexto relevante
    precise_answers = 0
    for i, context in enumerate(dataset["contexts"]):
        if context and len(context) > 0:
            # Verificar si el contexto es relevante
            if any("convocatoria" in str(ctx).lower() for ctx in context):
                precise_answers += 1
    
    # Context Recall: Respuestas que cubren la informaci√≥n necesaria
    recalled_answers = 0
    for i, answer in enumerate(dataset["answers"]):
        if answer and len(answer.strip()) > 20:
            recalled_answers += 1
    
    # Answer Relevancy: Respuestas relevantes a la pregunta
    relevant_answers = 0
    for i, (question, answer) in enumerate(zip(dataset["questions"], dataset["answers"])):
        if answer and len(answer.strip()) > 10:
            # Verificar si la respuesta es relevante a la pregunta
            if any(keyword in answer.lower() for keyword in question.lower().split()):
                relevant_answers += 1
    
    return {
        "faithfulness": faithful_answers / total_questions if total_questions > 0 else 0,
        "context_precision": precise_answers / total_questions if total_questions > 0 else 0,
        "context_recall": recalled_answers / total_questions if total_questions > 0 else 0,
        "answer_relevancy": relevant_answers / total_questions if total_questions > 0 else 0,
        "total_questions": total_questions,
        "valid_answers": valid_answers
    }

def main():
    """Funci√≥n principal para ejecutar la evaluaci√≥n"""
    
    print("üöÄ Iniciando evaluaci√≥n RAGAS para Semana 3")
    print("=" * 50)
    
    # Configuraciones a evaluar
    configurations = [
        {
            "name": "Baseline (Semana 2)",
            "use_preprocessing": False,
            "use_reranking": False,
            "use_query_rewriting": False
        },
        {
            "name": "Con Preprocesamiento",
            "use_preprocessing": True,
            "use_reranking": False,
            "use_query_rewriting": False
        },
        {
            "name": "Con Reranking",
            "use_preprocessing": False,
            "use_reranking": True,
            "use_query_rewriting": False
        },
        {
            "name": "Con Query Rewriting",
            "use_preprocessing": False,
            "use_reranking": False,
            "use_query_rewriting": True
        },
        {
            "name": "Completo (Semana 3)",
            "use_preprocessing": True,
            "use_reranking": True,
            "use_query_rewriting": True
        }
    ]
    
    results = {}
    
    # Evaluar cada configuraci√≥n
    for config in configurations:
        metrics = evaluate_configuration(
            config["name"],
            config["use_preprocessing"],
            config["use_reranking"],
            config["use_query_rewriting"]
        )
        
        if metrics:
            results[config["name"]] = metrics
            print(f"   üìä M√©tricas obtenidas:")
            print(f"      Faithfulness: {metrics['faithfulness']:.3f}")
            print(f"      Context Precision: {metrics['context_precision']:.3f}")
            print(f"      Context Recall: {metrics['context_recall']:.3f}")
            print(f"      Answer Relevancy: {metrics['answer_relevancy']:.3f}")
        else:
            print(f"   ‚ùå No se pudieron obtener m√©tricas para {config['name']}")
    
    # Guardar resultados
    output_file = Path(__file__).parent / "ragas_evaluation_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ Evaluaci√≥n completada. Resultados guardados en: {output_file}")
    
    # Mostrar resumen
    print("\nüìä RESUMEN DE RESULTADOS:")
    print("=" * 50)
    
    for config_name, metrics in results.items():
        print(f"\n{config_name}:")
        print(f"  Faithfulness: {metrics['faithfulness']:.3f}")
        print(f"  Context Precision: {metrics['context_precision']:.3f}")
        print(f"  Context Recall: {metrics['context_recall']:.3f}")
        print(f"  Answer Relevancy: {metrics['answer_relevancy']:.3f}")

if __name__ == "__main__":
    main()
