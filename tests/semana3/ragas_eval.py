"""
Evaluaci√≥n RAGAS para Semana 3
==============================

Este m√≥dulo implementa la evaluaci√≥n completa del sistema RAG
usando RAGAS para medir el impacto de las mejoras implementadas
en la Semana 3.
"""

import os
import sys
import json
import time
import yaml
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datasets import Dataset

# Agregar el directorio ra√≠z al path
sys.path.append(str(Path(__file__).parent.parent.parent))

from app.services.chunking_service import ChunkingService, ChunkingStrategy
from app.services.embedding_service import EmbeddingService
from app.services.retrieval_service import RetrievalService
from app.services.generation_service import GenerationService
from app.routers.ask import basic_rag_processing

# Importar m√©tricas RAGAS
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    answer_correctness,
    answer_similarity
)


class Semana3RagasEvaluator:
    """Evaluador RAGAS para la Semana 3."""
    
    def __init__(self, config_path: str = "tests/semana3/ragas_config.yml"):
        """
        Inicializa el evaluador con la configuraci√≥n.
        
        Args:
            config_path: Ruta al archivo de configuraci√≥n YAML
        """
        self.config_path = config_path
        self.config = self._load_config()
        self.results = {}
        
        # Inicializar servicios
        self.embedding_service = EmbeddingService()
        self.retrieval_service = RetrievalService()
        self.generation_service = GenerationService()
        
        print(f"üìä Evaluador RAGAS Semana 3 inicializado")
        print(f"üìã Configuraci√≥n cargada desde: {config_path}")
    
    def _load_config(self) -> Dict[str, Any]:
        """Carga la configuraci√≥n desde el archivo YAML."""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except Exception as e:
            print(f"‚ùå Error cargando configuraci√≥n: {str(e)}")
            return {}
    
    def _load_test_questions(self) -> List[Dict[str, Any]]:
        """Carga las preguntas de prueba."""
        questions_file = self.config.get("test_data", {}).get("questions_file", "sample_questions.json")
        questions_path = Path("tests/semana2") / questions_file
        
        try:
            with open(questions_path, 'r', encoding='utf-8') as f:
                questions = json.load(f)
            return questions
        except Exception as e:
            print(f"‚ö†Ô∏è Error cargando preguntas: {str(e)}")
            return []
    
    def _create_test_dataset(self, questions: List[Dict[str, Any]]) -> Dataset:
        """Crea el dataset de prueba para RAGAS."""
        dataset_data = {
            "question": [],
            "answer": [],
            "contexts": [],
            "ground_truth": []
        }
        
        for q in questions:
            dataset_data["question"].append(q["question"])
            dataset_data["answer"].append(q.get("answer", ""))
            dataset_data["contexts"].append(q.get("contexts", []))
            dataset_data["ground_truth"].append(q.get("ground_truth", ""))
        
        return Dataset.from_dict(dataset_data)
    
    def _get_metrics(self) -> List:
        """Obtiene las m√©tricas configuradas para la evaluaci√≥n."""
        metrics = []
        metrics_config = self.config.get("metrics", {})
        
        if metrics_config.get("faithfulness", {}).get("enabled", False):
            metrics.append(faithfulness)
        
        if metrics_config.get("context_precision", {}).get("enabled", False):
            metrics.append(context_precision)
        
        if metrics_config.get("context_recall", {}).get("enabled", False):
            metrics.append(context_recall)
        
        if metrics_config.get("answer_relevancy", {}).get("enabled", False):
            metrics.append(answer_relevancy)
        
        if metrics_config.get("answer_correctness", {}).get("enabled", False):
            metrics.append(answer_correctness)
        
        if metrics_config.get("answer_similarity", {}).get("enabled", False):
            metrics.append(answer_similarity)
        
        return metrics
    
    def evaluate_configuration(self, config_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Eval√∫a una configuraci√≥n espec√≠fica.
        
        Args:
            config_name: Nombre de la configuraci√≥n
            config: Par√°metros de configuraci√≥n
            
        Returns:
            Resultados de la evaluaci√≥n
        """
        print(f"\nüîÑ Evaluando configuraci√≥n: {config_name}")
        print(f"üìã Par√°metros: {config}")
        
        try:
            # Cargar preguntas de prueba
            questions = self._load_test_questions()
            if not questions:
                print("‚ö†Ô∏è No hay preguntas de prueba disponibles")
                return {}
            
            # Crear dataset para RAGAS
            dataset = self._create_test_dataset(questions)
            
            # Obtener m√©tricas
            metrics = self._get_metrics()
            
            print(f"üìä Evaluando con {len(metrics)} m√©tricas")
            print(f"üìù Dataset: {len(dataset)} ejemplos")
            
            # Ejecutar evaluaci√≥n
            start_time = time.time()
            results = evaluate(
                dataset=dataset,
                metrics=metrics,
                llm=self.generation_service.llm,
                embeddings=self.embedding_service.get_embeddings_model()
            )
            evaluation_time = time.time() - start_time
            
            # Procesar resultados
            evaluation_results = {
                "config_name": config_name,
                "config": config,
                "evaluation_time": evaluation_time,
                "total_samples": len(dataset),
                "metrics": {}
            }
            
            # Extraer m√©tricas individuales
            for metric in metrics:
                metric_name = metric.__name__
                if metric_name in results:
                    evaluation_results["metrics"][metric_name] = float(results[metric_name])
            
            print(f"‚úÖ Evaluaci√≥n completada en {evaluation_time:.2f} segundos")
            return evaluation_results
            
        except Exception as e:
            print(f"‚ùå Error en evaluaci√≥n: {str(e)}")
            return {
                "config_name": config_name,
                "config": config,
                "error": str(e),
                "metrics": {}
            }
    
    def run_comparative_evaluation(self) -> Dict[str, Any]:
        """Ejecuta la evaluaci√≥n comparativa entre Semana 2 y Semana 3."""
        print("\nüöÄ Iniciando evaluaci√≥n comparativa Semana 2 vs Semana 3")
        
        results = {}
        comparisons = self.config.get("comparisons", {})
        
        # Evaluaci√≥n Semana 2 (baseline)
        if comparisons.get("semana2_vs_semana3", {}).get("enabled", False):
            baseline_config = comparisons["semana2_vs_semana3"]["baseline"]["config"]
            baseline_results = self.evaluate_configuration("Semana 2", baseline_config)
            results["semana2"] = baseline_results
        
        # Evaluaci√≥n Semana 3 (mejorada)
        if comparisons.get("semana2_vs_semana3", {}).get("enabled", False):
            improved_config = comparisons["semana2_vs_semana3"]["improved"]["config"]
            improved_results = self.evaluate_configuration("Semana 3", improved_config)
            results["semana3"] = improved_results
        
        # Evaluaci√≥n de funcionalidades individuales
        if comparisons.get("individual_features", {}).get("enabled", False):
            individual_tests = comparisons["individual_features"]["tests"]
            for test in individual_tests:
                test_results = self.evaluate_configuration(test["name"], test["config"])
                results[test["name"]] = test_results
        
        return results
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """Genera un reporte de los resultados."""
        report = []
        report.append("# Reporte de Evaluaci√≥n RAGAS - Semana 3")
        report.append(f"üìÖ Fecha: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Resumen de configuraciones evaluadas
        report.append("## Configuraciones Evaluadas")
        for config_name, config_results in results.items():
            if "error" in config_results:
                report.append(f"- **{config_name}**: ‚ùå Error - {config_results['error']}")
            else:
                report.append(f"- **{config_name}**: ‚úÖ Completada")
        report.append("")
        
        # Comparaci√≥n de m√©tricas
        report.append("## Comparaci√≥n de M√©tricas")
        report.append("")
        
        # Crear tabla de m√©tricas
        metrics_table = []
        metrics_table.append("| Configuraci√≥n | Faithfulness | Context Precision | Context Recall | Answer Relevancy |")
        metrics_table.append("|---------------|--------------|-------------------|---------------|------------------|")
        
        for config_name, config_results in results.items():
            if "metrics" in config_results:
                metrics = config_results["metrics"]
                row = f"| {config_name} |"
                row += f" {metrics.get('faithfulness', 0):.3f} |"
                row += f" {metrics.get('context_precision', 0):.3f} |"
                row += f" {metrics.get('context_recall', 0):.3f} |"
                row += f" {metrics.get('answer_relevancy', 0):.3f} |"
                metrics_table.append(row)
        
        report.extend(metrics_table)
        report.append("")
        
        # An√°lisis de mejoras
        report.append("## An√°lisis de Mejoras")
        report.append("")
        
        if "semana2" in results and "semana3" in results:
            semana2_metrics = results["semana2"].get("metrics", {})
            semana3_metrics = results["semana3"].get("metrics", {})
            
            for metric_name in ["faithfulness", "context_precision", "context_recall", "answer_relevancy"]:
                if metric_name in semana2_metrics and metric_name in semana3_metrics:
                    improvement = semana3_metrics[metric_name] - semana2_metrics[metric_name]
                    if improvement > 0:
                        report.append(f"- **{metric_name}**: +{improvement:.3f} ‚úÖ Mejora")
                    elif improvement < 0:
                        report.append(f"- **{metric_name}**: {improvement:.3f} ‚ùå Regresi√≥n")
                    else:
                        report.append(f"- **{metric_name}**: Sin cambio")
        
        report.append("")
        
        # Recomendaciones
        report.append("## Recomendaciones")
        report.append("")
        report.append("1. **Preprocesamiento**: Eval√∫a si el uso de markitdown mejora la calidad de los fragmentos")
        report.append("2. **Reranking**: Verifica si el reranking mejora la relevancia de los documentos recuperados")
        report.append("3. **Query Rewriting**: Analiza si la reescritura de consultas mejora la recuperaci√≥n")
        report.append("4. **M√©tricas preocupantes**: Revisa m√©tricas por debajo de los umbrales configurados")
        
        return "\n".join(report)
    
    def save_results(self, results: Dict[str, Any], output_dir: str = "tests/semana3/results"):
        """Guarda los resultados en archivos."""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Guardar resultados JSON
        results_file = output_path / "evaluation_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # Generar y guardar reporte
        report = self.generate_report(results)
        report_file = output_path / "evaluation_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"üìÅ Resultados guardados en: {output_path}")
        print(f"üìÑ Reporte: {report_file}")
        print(f"üìä Datos: {results_file}")


def main():
    """Funci√≥n principal para ejecutar la evaluaci√≥n."""
    print("üöÄ Iniciando evaluaci√≥n RAGAS para Semana 3")
    
    # Crear evaluador
    evaluator = Semana3RagasEvaluator()
    
    # Ejecutar evaluaci√≥n comparativa
    results = evaluator.run_comparative_evaluation()
    
    # Generar reporte
    report = evaluator.generate_report(results)
    print("\n" + "="*50)
    print(report)
    print("="*50)
    
    # Guardar resultados
    evaluator.save_results(results)
    
    print("\n‚úÖ Evaluaci√≥n completada")


if __name__ == "__main__":
    main()
